{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzfPtlsLaSxZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMG_SIZE = 640\n",
        "CLOTH_IMAGE_PATH = '/content/drive/MyDrive/00070_00.jpg'\n",
        "STYLE_IMAGE_PATH = '/content/drive/MyDrive/art6.jpg'\n",
        "VGG_WEIGHTS_PATH = '/content/drive/MyDrive/vgg19-dcbb9e9d.pth'\n",
        "RESULT_SAVE_PATH = '/content/drive/MyDrive/styled_cloth_result3.jpg'\n",
        "\n",
        "def load_and_resize(path, size=256):\n",
        "    img = Image.open(path).convert('RGB').resize((size, size), Image.LANCZOS)\n",
        "    return img\n",
        "\n",
        "def pil_to_tensor(img):\n",
        "    return transforms.ToTensor()(img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "def tensor_to_pil(tensor):\n",
        "    t = tensor.clone().detach().cpu().squeeze(0)\n",
        "    t = (t * 255).clamp(0,255).byte()\n",
        "    return transforms.ToPILImage()(t)\n",
        "\n",
        "def get_features(x, model, layers):\n",
        "    feats = []\n",
        "    for idx, layer in enumerate(model):\n",
        "        x = layer(x)\n",
        "        if str(idx) in layers:\n",
        "            feats.append(x)\n",
        "    return feats\n",
        "\n",
        "def gram_matrix(t):\n",
        "    b, c, h, w = t.size()\n",
        "    f = t.view(b * c, h * w)\n",
        "    return (f @ f.t()) / f.numel()\n",
        "\n",
        "def resize_mask(mask, target_tensor):\n",
        "    return F.interpolate(mask, size=target_tensor.shape[2:], mode='nearest')\n",
        "\n",
        "def total_variation_loss(x):\n",
        "    return torch.mean(torch.abs(x[:,:,:,1:] - x[:,:,:,:-1])) + torch.mean(torch.abs(x[:,:,1:,:] - x[:,:,:-1,:]))\n",
        "\n",
        "def blur_mask(mask_tensor, sigma=2.5):\n",
        "    if mask_tensor.ndim == 4 and mask_tensor.shape[0] == 1 and mask_tensor.shape[1] == 1:\n",
        "        arr = mask_tensor.squeeze().cpu().numpy()\n",
        "        arr_blur = cv2.GaussianBlur(arr, (0, 0), sigma)\n",
        "        blur = torch.from_numpy(arr_blur).float().clamp(0, 1).to(mask_tensor.device)\n",
        "        return blur.unsqueeze(0).unsqueeze(0)\n",
        "    else:\n",
        "        return mask_tensor\n",
        "\n",
        "cloth_img = load_and_resize(CLOTH_IMAGE_PATH, IMG_SIZE)\n",
        "style_img = load_and_resize(STYLE_IMAGE_PATH, IMG_SIZE)\n",
        "content_tensor = pil_to_tensor(cloth_img)\n",
        "style_tensor = pil_to_tensor(style_img)\n",
        "\n",
        "yolo = YOLO('yolov8l-seg.pt')\n",
        "results = yolo(cloth_img, imgsz=IMG_SIZE, conf=0.3, iou=0.5)\n",
        "\n",
        "mask = None\n",
        "if results[0].masks is not None and len(results[0].masks.data) > 0:\n",
        "    mask_np = results[0].masks.data.cpu().numpy()[0]\n",
        "    mask = torch.from_numpy(mask_np).float().unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
        "    mask = mask / mask.max()\n",
        "    mask = blur_mask(mask, sigma=7.0)\n",
        "\n",
        "if mask is None:\n",
        "    print(\"Warning: No object detected by YOLOv8. Applying style transfer to the entire image.\")\n",
        "    mask = torch.ones_like(content_tensor[:, :1, :, :])\n",
        "\n",
        "vgg = models.vgg19(weights=None)\n",
        "vgg.load_state_dict(torch.load(VGG_WEIGHTS_PATH, map_location=DEVICE))\n",
        "vgg_features = vgg.features.to(DEVICE).eval()\n",
        "content_layers = [\"21\"]\n",
        "style_layers = [\"5\", \"10\", \"19\", \"28\"]\n",
        "\n",
        "with torch.no_grad():\n",
        "    content_feat = get_features(content_tensor, vgg_features, content_layers)[0]\n",
        "    style_feats = get_features(style_tensor, vgg_features, style_layers)\n",
        "    style_grams = [gram_matrix(f) for f in style_feats]\n",
        "\n",
        "input_img = content_tensor.clone().requires_grad_(True)\n",
        "optimizer = optim.Adam([input_img], lr=0.06)\n",
        "num_steps = 600\n",
        "alpha, beta, gamma = 1, 3e6, 2e-6\n",
        "\n",
        "for step in range(num_steps):\n",
        "    optimizer.zero_grad()\n",
        "    gen_style_feats = get_features(input_img, vgg_features, style_layers)\n",
        "    gen_content_feat = get_features(input_img, vgg_features, content_layers)[0]\n",
        "    c_mask = resize_mask(mask, gen_content_feat)\n",
        "    content_loss = alpha * F.mse_loss(gen_content_feat * c_mask, content_feat * c_mask)\n",
        "    style_loss = 0\n",
        "    for f, g in zip(gen_style_feats, style_grams):\n",
        "        s_mask = resize_mask(mask, f)\n",
        "        style_loss += F.mse_loss(gram_matrix(f * s_mask), g)\n",
        "    style_loss = beta * style_loss\n",
        "    tv_loss = gamma * total_variation_loss(input_img)\n",
        "    total_loss = content_loss + style_loss + tv_loss\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "        input_img.clamp_(0, 1)\n",
        "    if step % 20 == 0 or step == num_steps-1:\n",
        "        print(f'Step {step:3d}: Content {content_loss.item():.2f} | Style {style_loss.item():.2f} | TV {tv_loss.item():.5f}')\n",
        "\n",
        "with torch.no_grad():\n",
        "    stylized = input_img.detach()\n",
        "    mask_resz = resize_mask(mask, stylized).expand(-1, 3, -1, -1)\n",
        "    blended = stylized * mask_resz + content_tensor * (1 - mask_resz)\n",
        "\n",
        "result_pil = tensor_to_pil(blended)\n",
        "result_pil.save(RESULT_SAVE_PATH)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "images = [(cloth_img, \"Content Cloth\"), (style_img, \"Style\"), (result_pil, \"Stylized Cloth (Enhanced)\")]\n",
        "for idx, (img, title) in enumerate(images):\n",
        "    plt.subplot(1,3,idx+1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(f\"âœ… Enhanced stylized cloth saved to {RESULT_SAVE_PATH}\")\n"
      ]
    }
  ]
}